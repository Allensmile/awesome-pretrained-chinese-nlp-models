# Awesome Pretrained Chinese NLP Models[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

高质量中文预训练模型集合

[toc]

🤗[huggingface](https://github.com/huggingface/transformers)模型下载地址: 1. [清华大学开源镜像](https://mirror.tuna.tsinghua.edu.cn/hugging-face-models/) 2. [官方地址](https://huggingface.co/models)

 ## BERT
 
 paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
 
 > BERT-Base
 > 层数: 12
 > 数据大小； 词数0.4B
 > tensorflow; [Google Drive](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip)
 > Pytorch: 
 > 提供者：[Google Research](https://github.com/google-research)
 > 源地址：[link](https://github.com/google-research/bert)

 BERT-wwm
 层数: 12
 数据大小； 词数0.4B
 tensorflow; [Google Drive](https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW) [讯飞云-密码07Xj](http://pan.iflytek.com/#/link/A2483AD206EF85FD91569B498A3C3879)
 Pytorch: [Google Drive](https://drive.google.com/open?id=1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY)
 提供者：[Yiming Cui](https://github.com/ymcui)
 源地址：[link](https://github.com/ymcui/Chinese-BERT-wwm)

 BERT-wwm-ext
 层数: 12
 数据大小； 词数5.4B
 tensorflow; [Google Drive](https://drive.google.com/open?id=1buMLEjdtrXE2c4G1rpsNGWEx7lUQ0RHi) [讯飞云-密码4cMG](http://pan.iflytek.com/#/link/653637473FFF242C3869D77026C9BDB5)
 Pytorch: [Google Drive](https://drive.google.com/open?id=1iNeYFhCBJWeUsIlnW_2K6SMwXkM4gLb_)
 提供者：[Yiming Cui](https://github.com/ymcui)
 源地址：[link](https://github.com/ymcui/Chinese-BERT-wwm)

